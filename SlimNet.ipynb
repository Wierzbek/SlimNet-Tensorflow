{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import SeparableConv2D\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution with Batch Norm & ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBNReLU(filters, kernel_size, x, strides=1,):\n",
    "    if(kernel_size == 1):\n",
    "        padding = 0\n",
    "    else:\n",
    "        padding = 1        \n",
    "    # Conv2D\n",
    "    x = ZeroPadding2D(padding)(x)\n",
    "    x = Conv2D(filters, kernel_size, strides)(x)\n",
    "    # Batch Normalization\n",
    "    x = BatchNormalization()(x)\n",
    "    # Activation Function\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depthwise Separable Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depthwise_separable(filters, kernel_size, x):\n",
    "    #add padding\n",
    "    x = ZeroPadding2D(padding=1)(x)\n",
    "    # Depthwise Convolution 3x3\n",
    "    x = SeparableConv2D(filters, kernel_size)(x)\n",
    "    # Batch Normalization\n",
    "    x = BatchNormalization()(x)\n",
    "    # Activation Function\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separable Squeeze-Expand Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSEModel(squeeze, expand, x):\n",
    "    #squeeze\n",
    "    pointwise1 = ConvBNReLU(squeeze, 1, x)\n",
    "    #expand\n",
    "    pointwise2 = ConvBNReLU(expand, 1, pointwise1)\n",
    "    depthwise = depthwise_separable(expand, 3, pointwise1)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([pointwise2, depthwise])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slim Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SlimModule(squeeze, expand, dws, filtersNo, x):\n",
    "    #SSE block 1\n",
    "    SSE1 = SSEModel(squeeze, expand, x)\n",
    "    #skip connection\n",
    "    SkipConnection = 0\n",
    "    #Dense\n",
    "    dns = Dense(filtersNo, activation='relu')(SSE1)\n",
    "    \n",
    "    SkipConnection = Add()([x, dns])\n",
    "    #SSE block 2\n",
    "    SSE2 = SSEModel(squeeze, expand, SkipConnection)\n",
    "\n",
    "    #depthwise separable conv\n",
    "    x = depthwise_separable(dws, 3, SSE2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SlimNet - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(224, 224, 3))\n",
    "x = Conv2D(96, (7, 7), (2, 2), padding='same')(inp)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#Slim module 1\n",
    "x = SlimModule(16, 64, 48, 96, x)\n",
    "x = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "#Slim module 2\n",
    "x = SlimModule(32, 128, 96, 48, x)\n",
    "x = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "#Slim module 3\n",
    "x = Dropout(0.5)(x)\n",
    "x = SlimModule(48, 192, 144, 96, x)\n",
    "x = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "#Slim module 4\n",
    "x = Dropout(0.5)(x)\n",
    "x = SlimModule(64, 256, 192, 144, x)\n",
    "x = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "x = tf.keras.Model(inputs=inp, outputs = x)\n",
    "\n",
    "print(x.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# create generator\n",
    "datagen = ImageDataGenerator()\n",
    "# prepare an iterators for each dataset\n",
    "train_it = datagen.flow_from_directory('../CelebA dataset/train', target_size=(96, 96), class_mode='binary')\n",
    "val_it = datagen.flow_from_directory('../CelebA dataset/val', target_size=(96, 96), class_mode='binary')\n",
    "#test_it = datagen.flow_from_directory('./faces', target_size=(128, 128), class_mode='categorical')\n",
    "# confirm the iterator work\n",
    "batchX, batchy = train_it.next()\n",
    "print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./SlimModel/Checkppoint/SlimNet_Checkpoint.h5\",\n",
    "                            monitor=\"val_loss\",\n",
    "                            mode=\"min\",\n",
    "                            save_best_only = True,\n",
    "                            verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = \"val_loss\",\n",
    "                         min_delta = 0,\n",
    "                         patience = 3,\n",
    "                         verbose = 1,\n",
    "                         restore_best_weights = True)\n",
    "\n",
    "callbacks = [earlystop, checkpoint]\n",
    "\n",
    "opt = tensorflow.keras.optimizers.Adam(lr=0.01e-3, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "x.compile(loss = 'binary_crossentropy',\n",
    "             optimizer = opt,\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 49893\n",
    "nb_validation_samples = 6236\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 300\n",
    "\n",
    "\n",
    "history = x.fit(\n",
    "    train_it,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data = val_it,\n",
    "    validation_steps = nb_validation_samples // batch_size)\n",
    "\n",
    "\n",
    "x.save(\"./SlimModel/SlimNet_Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the exact same model, including its weights and the optimizer\n",
    "new_model = tf.keras.models.load_model(\"./SlimModel/SlimNet_Model.h5\")\n",
    "\n",
    "# Show the model architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss, acc = new_model.evaluate(val_it, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Loads image from path and resizes it\"\"\"\n",
    "    img = load_img(image_path, target_size=(128, 128))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def loadSlimFaceModel():\n",
    "    \"\"\"Loads the SlimNetFace model defined in the function\"\"\"\n",
    " \n",
    "    inp = Input(shape=(128, 128, 3))\n",
    "    x = Conv2D(96, (7, 7), (2, 2), padding='same')(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    #Slim module 1\n",
    "    x = SlimModule(16, 64, 48, 96, x)\n",
    "    x = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    #Slim module 2\n",
    "    x = SlimModule(32, 128, 96, 48, x)\n",
    "    x = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    #Slim module 3\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = SlimModule(48, 192, 144, 96, x)\n",
    "    x = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    #Slim module 4\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = SlimModule(64, 256, 192, 144, x)\n",
    "    x = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    \n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    x = tf.keras.Model(inputs=inp, outputs = x)\n",
    "\n",
    "    from keras.models import model_from_json\n",
    "    x.load_weights('./SlimModel/Checkppoint/SlimNet_Checkpoint.h5')\n",
    "\n",
    "    SlimNet_face_descriptor = Model(inputs=x.layers[0].input, outputs=x.layers[-2].output)\n",
    "\n",
    "    return SlimNet_face_descriptor\n",
    "\n",
    "model = loadSlimFaceModel()\n",
    "print(\"Model Loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on a mp4 vide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import cv2\n",
    "\n",
    "# Loading HAARCascade Face Detector \n",
    "face_detector = cv2.CascadeClassifier('Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Directory of image of persons we'll be extracting faces from\n",
    "mypath = \"./friends/\"\n",
    "image_file_names = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "print(\"Collected image names\")\n",
    "\n",
    "for image_name in image_file_names:\n",
    "    person_image = cv2.imread(mypath+image_name)\n",
    "    face_info = face_detector.detectMultiScale(person_image, 1.3, 5)\n",
    "    for (x,y,w,h) in face_info:\n",
    "        face = person_image[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(face, (128, 128), interpolation = cv2.INTER_CUBIC)\n",
    "    path = \"./friends_faces/\" + \"face_\" + image_name \n",
    "    cv2.imwrite(path, roi)\n",
    "    cv2.imshow(\"face\", roi)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We load our faces from the \"friends_faces\" directory and we run our face classifier model our test video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points to your extracted faces\n",
    "people_pictures = \"./friends_faces/\"\n",
    "\n",
    "all_people_faces = dict()\n",
    "\n",
    "for file in listdir(people_pictures):\n",
    "    person_face, extension = file.split(\".\")\n",
    "    all_people_faces[person_face] = model.predict(preprocess_image('./friends_faces/%s.jpg' % (person_face)))[0,:]\n",
    "\n",
    "print(\"Face representations retrieved successfully\")\n",
    "\n",
    "def findCosineSimilarity(source_representation, test_representation):\n",
    "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
    "    b = np.sum(np.multiply(source_representation, source_representation))\n",
    "    c = np.sum(np.multiply(test_representation, test_representation))\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
    "\n",
    "#Open Webcam or video\n",
    "#cap = cv2.VideoCapture(0) \n",
    "cap = cv2.VideoCapture('testfriends.mp4')\n",
    "\n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    img = cv2.resize(img, (640, 360)) # Re-size video to as smaller size to improve face detection speed\n",
    "    faces = face_detector.detectMultiScale(img, 1.3, 5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        if w > 13: \n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) #draw rectangle to main image\n",
    "\n",
    "            detected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
    "            detected_face = cv2.resize(detected_face, (128, 128)) #resize to 224x224\n",
    "\n",
    "            img_pixels = image.img_to_array(detected_face)\n",
    "            img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "            img_pixels /= 255\n",
    "\n",
    "            captured_representation = model.predict(img_pixels)[0,:]\n",
    "\n",
    "            found = 0\n",
    "            for i in all_people_faces:\n",
    "                person_name = i\n",
    "                representation = all_people_faces[i]\n",
    "\n",
    "                similarity = findCosineSimilarity(representation, captured_representation)\n",
    "                if(similarity < 0.30):\n",
    "                    cv2.putText(img, person_name[5:], (int(x+w+15), int(y-12)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "                    found = 1\n",
    "                    break\n",
    "\n",
    "            #connect face and text\n",
    "            cv2.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),(255, 0, 0),1)\n",
    "            cv2.line(img,(x+w,y-20),(x+w+10,y-20),(255, 0, 0),1)\n",
    "\n",
    "            if(found == 0): #if found image is not in our people database\n",
    "                cv2.putText(img, 'unknown', (int(x+w+15), int(y-12)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow('img',img)\n",
    "\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "\n",
    "#kill open cv things\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
